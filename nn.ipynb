{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "np.random.seed(113) #set seed before any keras import\n",
    "import argparse\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, LSTM, Dropout\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "with open('data/us.text') as f: \n",
    "    X_es = f.readlines()\n",
    "with open('data/us.labels') as f: \n",
    "    Y_es = f.readlines()\n",
    "    \n",
    "assert len(X_es) == len(Y_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_es, Y_es, test_size=0.9, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:LoL @ West Covina, California \n",
      "\n",
      "INFO:root:2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logging.info(X_es[0])\n",
    "logging.info(Y_es[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding\n",
    "# label2idx = {label: i for i, label in enumerate(set(y_train))}\n",
    "# num_labels = len(label2idx.keys())\n",
    "# y_train = np_utils.to_categorical([label2idx[label] for label in y_train], num_classes=num_labels)\n",
    "# y_test = np_utils.to_categorical([label2idx[label] for label in y_test], num_classes=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "y2i = defaultdict(lambda: len(y2i))\n",
    "y_train_num = [y2i[emoji] for emoji in y_train]\n",
    "y_test_num = [y2i[emoji] for emoji in y_test]\n",
    "num_classes = len(np.unique(y_train_num))\n",
    "print(num_classes)\n",
    "\n",
    "y_train_one_hot = np_utils.to_categorical(y_train_num, num_classes) #important to give the num_classes!\n",
    "y_test_one_hot = np_utils.to_categorical(y_test_num, num_classes) #important to give the num_classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_one_hot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "PAD = w2i[\"<pad>\"] # index 0 is padding\n",
    "UNK = w2i[\"<unk>\"] # index 1 is for UNK\n",
    "\n",
    "# convert words to indices, taking care of UNKs\n",
    "X_train_num = [[w2i[word] for word in sentence.split(\" \")] for sentence in X_train]\n",
    "w2i = defaultdict(lambda: UNK, w2i) # freeze - cute trick!\n",
    "X_test_num = [[w2i[word] for word in sentence.split(\" \")] for sentence in X_test]\n",
    "\n",
    "max_sentence_length=max([len(s.split(\" \")) for s in X_train] \n",
    "                        + [len(s.split(\" \")) for s in X_test] )\n",
    "print(max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 3, 5, 6]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_num[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49026, 43)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "# pad X\n",
    "X_train_pad = sequence.pad_sequences(X_train_num, maxlen=max_sentence_length, value=PAD)\n",
    "X_test_pad = sequence.pad_sequences(X_test_num, maxlen=max_sentence_length,value=PAD)\n",
    "print(X_train_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(w2i)\n",
    "embeds_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(113) #set seed before any keras import\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embeds_size, input_length=max_sentence_length))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "49026/49026 [==============================] - 336s 7ms/step - loss: 2.4840 - acc: 0.2607\n",
      "Epoch 2/3\n",
      "49026/49026 [==============================] - 325s 7ms/step - loss: 2.0342 - acc: 0.3751\n",
      "Epoch 3/3\n",
      "49026/49026 [==============================] - 315s 6ms/step - loss: 1.4861 - acc: 0.5401\n",
      "441239/441239 [==============================] - 67s 153us/step\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_pad, y_train_one_hot, epochs=3)\n",
    "loss, accuracy = model.evaluate(X_test_pad, y_test_one_hot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appliedml",
   "language": "python",
   "name": "appliedml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
